# Distributed Model Training Using Ray
This project demonstrates distributed neural network training using Ray, a Python framework for distributed computing. The goal is to explore how distributed processing can optimize deep learning workflows by running multiple configurations of a neural network in parallel.

Features
Implements distributed model training using Ray.
Compares different configurations of filters and dense layers.
Logs the performance (loss and accuracy) of each configuration.

Key Results
Filters: 32, Dense Units: 64, Loss: 0.0599, Accuracy: 98.1200%
Filters: 64, Dense Units: 128, Loss: 0.0477, Accuracy: 98.4000%
Filters: 128, Dense Units: 256, Loss: 0.0392, Accuracy: 98.7100%
